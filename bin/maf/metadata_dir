#!/usr/bin/env python

import os
import argparse
import glob
import numpy as np
import shutil
import matplotlib

matplotlib.use("Agg")

from rubin_sim.maf.metrics import CountExplimMetric
from rubin_sim.maf.slicers import HealpixSlicer, HealpixSubsetSlicer
from rubin_sim.maf.metricBundles import MetricBundle, MetricBundleGroup
from rubin_sim.maf.utils import getDateVersion
from rubin_sim.maf.db import TrackingDb, ResultsDb
import rubin_sim.maf.batches as batches


if __name__ == "__main__":
    """
    Run the metadata batch on all .db files in a directory.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--db", type=str, default=None)
    parser.add_argument(
        "--nside",
        type=float,
        default=64,
        help="nside to use for the healpix slicer and subsetslicer for metrics.",
    )
    parser.add_argument(
        "--wfd_threshold",
        type=float,
        default=750,
        help="Threshold number of visits per pointing to use to define the WFD footprint."
        "Default value of 750 corresponds to the minimum median value per pointing from the SRD.",
    )
    args = parser.parse_args()

    # If runNames not given, scan for opsim databases in current directory and use those
    # Note that 'runNames' can be full path to directories

    if args.db is None:
        # Just look for any .db files in this directory
        dbFiles = glob.glob("*.db")
        # But remove trackingDb and resultsDb if they're there
        try:
            dbFiles.remove("trackingDb_sqlite.db")
        except ValueError:
            pass
        try:
            dbFiles.remove("resultsDb_sqlite.db")
        except ValueError:
            pass
    elif isinstance(args.db, str):
        dbFiles = [args.db]
    else:
        dbFiles = args.db

    sim_names = [os.path.basename(name).replace(".db", "") for name in dbFiles]

    trackingDb = TrackingDb(database=None)
    mafDate, mafVersion = getDateVersion()
    mafVersion = mafVersion["__version__"]

    for filename, opsim in zip(dbFiles, sim_names):
        # Connect to the database
        opsdb = filename
        colmap = batches.ColMapDict()

        # Set and create if needed the output directory
        # I guess 'meta' really means to be more like 'conditions' of visits here.
        outDir = opsim + "_meta"
        if os.path.isdir(outDir):
            shutil.rmtree(outDir)

        # Find the 'wfd' footprint
        m = CountExplimMetric(col="observationStartMJD")
        allsky_slicer = HealpixSlicer(nside=args.nside)
        constraint = 'note not like "%DD%"'
        bundle = MetricBundle(m, allsky_slicer, constraint, runName=opsim)
        g = MetricBundleGroup({f"{opsim} footprint": bundle}, opsdb, outDir=outDir)
        g.runAll()
        wfd_footprint = bundle.metricValues.filled(0)
        wfd_footprint = np.where(wfd_footprint > args.wfd_threshold, 1, 0)
        wfd_hpix = np.where(wfd_footprint == 1)[0]
        wfd_slicer = HealpixSubsetSlicer(nside=args.nside, hpid=wfd_hpix)

        # Set up the bundle dicts
        # Some of these metrics are reproduced in other scripts - srd and cadence
        bdict = {}

        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            fO = batches.fOBatch(
                colmap=colmap, runName=opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(fO)
            astrometry = batches.astrometryBatch(
                colmap=colmap, runName=opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(astrometry)
            rapidrevisit = batches.rapidRevisitBatch(
                colmap=colmap, runName=opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(rapidrevisit)

        # Intranight (pairs/time)
        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            intranight = batches.intraNight(
                colmap, opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(intranight)

        # Internight (nights between visits)
        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            internight = batches.interNight(
                colmap, opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(internight)

        # Intraseason (length of season)
        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            season = batches.seasons(
                colmap=colmap, runName=opsim, slicer=slicer, extraInfoLabel=tag
            )
            bdict.update(season)

        # Run all metadata metrics, All and just WFD.
        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            bdict.update(
                batches.allMetadata(colmap, opsim, slicer=slicer, extraInfoLabel=tag)
            )

        # Nvisits + m5 maps + Teff maps, All and just WFD.
        for tag, slicer in zip(["All sky", "WFD"], [allsky_slicer, wfd_slicer]):
            bdict.update(
                batches.nvisitsM5Maps(colmap, opsim, slicer=slicer, extraInfoLabel=tag)
            )
            bdict.update(
                batches.tEffMetrics(colmap, opsim, slicer=slicer, extraInfoLabel=tag)
            )

        # Nvisits per proposal and per night.
        ### NEED MORE HERE
        # bdict.update(batches.nvisitsPerProp(opsdb, colmap, opsim,
        #                                    slicer=allsky_slicer))

        # NVisits alt/az LambertSkyMap (all filters, per filter)
        bdict.update(batches.altazLambert(colmap, opsim))

        # Slew metrics.
        bdict.update(batches.slewBasics(colmap, opsim))

        # Open shutter metrics.
        bdict.update(batches.openshutterFractions(colmap, opsim))

        # Per night and whole survey filter changes.
        bdict.update(batches.filtersPerNight(colmap, opsim, nights=1, extraSql=None))
        bdict.update(batches.filtersWholeSurvey(colmap, opsim, extraSql=None))

        # Set up the resultsDB
        resultsDb = ResultsDb(outDir=outDir)
        # Go and run it
        group = MetricBundleGroup(
            bdict, opsdb, outDir=outDir, resultsDb=resultsDb, saveEarly=False
        )
        group.runAll(clearMemory=True, plotNow=True)
        resultsDb.close()

        # Add outputs to tracking database. -- note possible race condition if running in parallel.
        trackingDb.addRun(
            opsimRun=opsim,
            opsimVersion=None,
            opsimDate=None,
            mafComment="Metadata",
            mafVersion=mafVersion,
            mafDate=mafDate,
            mafDir=outDir,
            dbFile=filename,
            mafRunId=None,
            opsimGroup=None,
            opsimComment=None,
        )
    # Close trackingDB
    trackingDb.close()
